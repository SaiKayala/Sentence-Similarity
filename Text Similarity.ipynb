{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "s2=\"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\n",
    "s3=\"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preProcess(strings):\n",
    "    '''\n",
    "    Input: All documents(as strings)\n",
    "    \n",
    "    Pre-Process given documents:\n",
    "    Consider only words and ignore non-alphabetic(integers)\n",
    "    Filter all the Stop-words\n",
    "    Ignore all the punctuations\n",
    "    \n",
    "    Return: Filtered and pre-processed list of words for all documents\n",
    "    '''\n",
    "    stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]    \n",
    "    samples=[]\n",
    "    for i in strings:\n",
    "        temp=re.sub(r'[^\\w\\s]', '', i).lower().split()\n",
    "        temp = [w for w in temp if not w in stopwords]\n",
    "        samples.append(temp)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getUniqueWords(samples):\n",
    "    '''\n",
    "    Input: All the pre-processed words\n",
    "    Return: Bag of unique words contained in all the documents\n",
    "    '''\n",
    "    unique_words=[]\n",
    "    for i in samples:\n",
    "        unique_words.extend(i)\n",
    "    unique_words=set(unique_words)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBagofWords(unique_words,samples):\n",
    "    '''\n",
    "    Input: Set of Unique words in all documets and Pre-Processed Words for each document\n",
    "    \n",
    "    Creates a  bag of word dictionary for each document with keys as unique words in all documents\n",
    "    \n",
    "    Return: Bag of words dictionary for each document\n",
    "    '''\n",
    "    samples_dicts=[]\n",
    "    for i in range(len(samples)):\n",
    "        temp=dict.fromkeys(unique_words, 0)\n",
    "        for word in samples[i]:\n",
    "            temp[word]+=1\n",
    "        samples_dicts.append(temp)\n",
    "    return samples_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(samples_dict, samples):\n",
    "    '''\n",
    "    Input: Bag of words dictionary of each sample wrt to unique words, Pre-Processed words\n",
    "    \n",
    "    Computes term-frequency(TF) for each document as a dictionary\n",
    "    \n",
    "    Return: List of Term-Frequency dictionaries \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    tf_dicts=[]\n",
    "    for i in range(len(samples)):\n",
    "        tfDict = {}\n",
    "        sampleCount = len(samples[i])\n",
    "        for word, count in samples_dict[i].items():\n",
    "            tfDict[word] = count / float(sampleCount)\n",
    "        tf_dicts.append(tfDict)\n",
    "    return tf_dicts\n",
    "\n",
    "def computeIDF(samples_dicts,unique_words):\n",
    "    '''\n",
    "    Input: Bag of words dictionary of each sample wrt to unique words, Set of unique words\n",
    "    \n",
    "    Computes Inverse Document-frequency(IDF) for a given list of documents as a dictionary\n",
    "    \n",
    "    Return: Dictionary of Inverse Document Frequency\n",
    "    \n",
    "    '''\n",
    "    idf_dict=dict.fromkeys(unique_words, 0)\n",
    "    N=len(samples_dicts)\n",
    "    for w in unique_words:\n",
    "        for i in samples_dicts:\n",
    "            if i[w]>0:\n",
    "                idf_dict[w]+=1\n",
    "\n",
    "    for word, val in idf_dict.items():\n",
    "            idf_dict[word] = math.log(N +1/ float(val))\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tf_dicts, idf_dict):\n",
    "    '''\n",
    "    Input: List if Term-Frequency dictionaries, Inverse Document frequency dictionary\n",
    "    \n",
    "    Computes Term frequency Inverse Document frequency(TFIDF) for a given list of documents as a dictionary\n",
    "    \n",
    "    Return: Dictionary of TFIDF\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    tfidf_list=[]\n",
    "    for i in range(len(tf_dicts)):\n",
    "        tfidf = {}\n",
    "        for word, val in tf_dicts[i].items():\n",
    "            tfidf[word] = val * idf_dict[word]\n",
    "        tfidf_list.append(tfidf)\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(doc1,doc2):\n",
    "    '''\n",
    "    Input: TFIDF dictionaries for the documents\n",
    "    \n",
    "    Computes Cosine similarity based on tfidf vectors \n",
    "    \n",
    "    Return: Cosine-Similarity between two documents\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    v1=list(doc1.values())\n",
    "    v2=list(doc2.values())\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(doc1,doc2):\n",
    "    '''\n",
    "    Input: 2 Strings/Documents\n",
    "    \n",
    "    End-to-End process of preprocessing of strings and computing respective TFIDF componets and similarity measure\n",
    "    \n",
    "    Return: Similarity(Cosine) measure between them\n",
    "    \n",
    "    '''\n",
    "    s1,s2=doc1,doc2\n",
    "    strings=[s1,s2]\n",
    "    samples=preProcess(strings)\n",
    "    unique_words=getUniqueWords(samples)\n",
    "    sample_dicts=createBagofWords(unique_words,samples)\n",
    "    tf_dicts=computeTF(sample_dicts,samples)\n",
    "    idf_dict=computeIDF(sample_dicts,unique_words)\n",
    "    tfidf_list=computeTFIDF(tf_dicts,idf_dict)\n",
    "    cosineSimilarty=cosine_similarity(tfidf_list[0],tfidf_list[1])\n",
    "    return cosineSimilarty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Between Samples 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7273635746759304"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeSimilarity(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Between Samples 1 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21560898744669124"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeSimilarity(s1,s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Between Samples 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20267265030036252"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeSimilarity(s2,s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
